{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 08 웹 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 웹 데이터 자동 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML 소스 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬 웹 크롤링 패키지\n",
    "#### [ requests ]\n",
    "- 브라우저 없이 URL에 접속, 결과값(HTML)을 추출함\n",
    "- 웹 브라우저를 실행하지 않고 특정 URL에 요청(request)을 보낼 수 있음\n",
    "- 응답(response)된 데이터의 헤더(Header), 바디(HTML) 등의 정보 확인\n",
    "- 빠르고 안정적이지만 동적 웹 페이지를 크롤링하기 위해서는 많은 분석이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ selenium ]\n",
    "- 브라우저를 자동으로 실행해 URL에 접속, 결과값(HTML)을 추출함\n",
    "- 시뮬레이션하듯이 웹 브라우저를 직접 실행하여 URL에 접속할 수 있음\n",
    "- 응답된 데이터의 헤더(Header), 바디(HTML) 등의 정보를 확인할 수 있음\n",
    "- 느리고 불안정하지만 동적 웹 페이지 크롤링이 쉽고 초보자가 사용하기 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ BeautifulSoup ] \n",
    "- requests와 selenium으로 받아온 내용(HTML)을 분석\n",
    "- requests나 selenium을 통해 얻은 HTML 데이터를 파싱(parsing)해 객체로 구조화\n",
    "- HTML 수집은 앞선 두 모듈이 담당하지만, 분석은 BeautifulSoup이 담당\n",
    "- HTML 태그와 CSS 속성을 이용해 원하는 정보를 찾을 수 있는 기능 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                                              # requests 모듈 불러오기\n",
    "res = requests.get(\"http://www.naver.com\", verify = False)   # GET 방식으로 네이버 홈페이지 가져오기\n",
    "print(res.content)                                            # HTML 형식의 데이터 출력\n",
    "\n",
    "# verify를 False로 지정하면 SSL인증서 오류 문제가 발생해도 URL에 입력한 웹 페이지를 가져올 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET 방식\n",
    "parameters = {\"code\" : \"005930\"}     # 삼성전자 종목코드\n",
    "res = requests.get(\"https://finance.naver.com/item/sise.nhn\", params=parameters, verify = False)\n",
    "print(res.url)\n",
    "print(res.text)    # 한글 출력이 가능하여 텍스트 위주의 웹 페이지일 경우 유용하게 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"https://finance.naver.com/item/sise.nhn?code=005930\", verify = False)\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST 방식\n",
    "res = requests.post(\"https://finance.naver.com/item/sise.nhn = 005930\", verify = False)\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML 소스를 데이터로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup                    # BeautifulSoup 모듈 불러오기\n",
    "\n",
    "# html_txt 변수에 HTML 소스와 텍스트 입력\n",
    "html_txt = \"<p class='wether' id='tw'>오늘의 날씨</p>  <h1> 한때 소나기가 내리겠습니다.</h1>\"\n",
    "\n",
    "soup = BeautifulSoup(html_txt,\"html.parser\")      # 파싱하기, 분석 가능한 데이터 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find(\"p\")        # p 태그를 찾아 tag에 저장\n",
    "\n",
    "print(tag)                  # tag 출력\n",
    "print(tag.name)             # tag명 출력\n",
    "print(tag.attrs )           # tag 속성 출력\n",
    "print(tag.attrs[\"class\"])   # tag 속성 중 class만 출력\n",
    "print(tag.attrs[\"id\"])      # tag 속성 중 id만 출력\n",
    "print(tag.text)             # tag 내 텍스트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML 소스 생성\n",
    "html_txt = \"\"\"                \n",
    "<html>\n",
    "<head><title>BS page</title></head>\n",
    "<body>\n",
    "\n",
    "<h1 class=\"portal_cls\">검색포털</h1>\n",
    "<p>\n",
    "<a href=\"http://www.daum.net\">다음 바로가기</a><br>\n",
    "<a href=\"http://www.naver.com\">네이버 바로가기</a>\n",
    "</p>\n",
    "<h1 class=\"portal_cls\">검색엔진</h1>\n",
    "<a href=\"http://www.google.com\" class=\"alink_cls\">구글</a>\n",
    "<p class=\"footage_cls\" id=\"company\">2021, ABC Company </p>\n",
    "<p class=\"footage_cls\" id=\"addr\">Korea</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html_txt, \"html.parser\")       # 파싱하기, 분석 가능한 데이터 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 태그 찾기\n",
    "tag = soup.select_one(\"h1\")                 # <h1> </h1> 태그 추출\n",
    "print(tag.text)                             # tag 안에 있는 텍스트만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = soup.select(\"h1\")                # <h1> ~ </h1> 태그 모두 찾기\n",
    "for tag in tag_list:                       # tag_list 내 데이터를 하나씩 읽어오기\n",
    "    print(tag.text)                        # 텍스트만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자식/자손 태그 찾기\n",
    "tag_list = soup.select(\"body p > a\")       # <body><p> 태그 하위의 <a> 태그 추출\n",
    "for tag in tag_list:                       # tag_list 내 데이터를 하나씩 읽어오기\n",
    "    print(tag.text)                       # 텍스트만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS ID로 태그 찾기\n",
    "tag_list = soup.select(\".footage_cls\")    # footage_cls 클래스 추출\n",
    "for tag in tag_list:                     # tag_list 내 데이터를 하나씩 읽어오기\n",
    "    print(tag.text)                      # 텍스트만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그 속성값을 이용해서 태그 찾기\n",
    "tag_list = soup.select(\"a[href]\")        # a[href] 추출\n",
    "for tag in tag_list:                    # tag_list 내 데이터를 하나씩 읽어오기\n",
    "    print(tag.text, tag.attrs[\"href\"])  # 텍스트와 URL 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] 서점 베스트셀러 정보 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "res = requests.get(\"http://www.kyobobook.co.kr/bestSellerNew/bestseller.laf?range = \\1&kind=2&orderClick = DAB&mallGb=KOR&linkClass=B\")\n",
    "html = res.content\n",
    "book = bs(html, \"html.parser\")\n",
    "\n",
    "book_list = []\n",
    "for book_detail in book.select(\"div.info\"):\n",
    "    book_urls = book_detail.select_one(\"div.title > a > span\").attrs[\"href\"]\n",
    "    book_list.append(book_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "res = requests.get(\"https://product.kyobobook.co.kr/bestseller/total#?page=1&per=20&period=002&ymw=&bsslBksClstCode=B\")\n",
    "html = res.content\n",
    "book = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 불러오기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "# 교보문고의 월간 베스트셀러(소설) 웹 페이지 가져오기\n",
    "res = requests.get(\"http://www.kyobobook.co.kr/bestSellerNew/bestseller.laf?range = \\1&kind=2&orderClick = DAB&mallGb=KOR&linkClass=B\")\n",
    "html = res.content                              # 웹 리소스 가져오기\n",
    "book = bs(html, \"html.parser\")                  # HTML 파싱\n",
    "\n",
    "# 도서 상세 페이지 주소 추출하기\n",
    "book_list = []                                  # book_list 리스트 변수 만들기\n",
    "for book_detail in book.select(\"div.detail\"):  # book에서 class=detail 태그 가져오기\n",
    "    book_urls = book_detail.select_one(\"div.title > a\").attrs[\"href\"]   # 상세 주소를 book_urls에 저장\n",
    "    book_list.append(book_urls)                 # book_list 변수에 주소를 하나씩 추가\n",
    "\n",
    "# 도서별 상세 데이터 추출하기\n",
    "best = pd.DataFrame(columns=[\"도서\",\"저자\",\"가격\",\"url\"])     # 저장할 데이터프레임 만들기\n",
    "for index, book_url in enumerate(book_list):                 # 도서 상세 페이지를 하나씩 반환\n",
    "    res = requests.get(book_url)                             # 도서 상세 페이지 리소스 가져오기\n",
    "    html = res.content\n",
    "    best_book = bs(html, \"html.parser\", from_encoding = \"cp949\")        # HTML 파싱\n",
    "    title = best_book.select_one(\"h1.title strong\").text.strip()        # 도서명 추출\n",
    "    author = best_book.select_one(\"span.name a\").text.strip()           # 저자명 추출\n",
    "    price = best_book.select_one(\"span.sell_price strong\").text.strip() # 가격 추출    \n",
    "    best.loc[index+1] = (title, author, price, book_url)                # 데이터 프레임에 저장\n",
    "\n",
    "# 엑셀 파일로 저장하기\n",
    "best.to_excel(\"bestseller.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 웹 브라우저 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selenium 개요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 로드 및 HTML 소스 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium                      # selenium 모듈 불러오기\n",
    "from selenium import webdriver      # webdriver 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.naver.com\"               # 네이버 URL을 URL 변수에 저장\n",
    "driver = webdriver.Chrome(\"chromedriver\")   # driver 객체 생성 \n",
    "driver.get(URL)                             # URL에 저장된 주소를 크롬에서 열기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver.current_url)\t\t\t\t# 현재 URL 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium                              # selenium 모듈 불러오기\n",
    "from bs4 import BeautifulSoup               # BeautifulSoup 모듈 불러오기\n",
    "from selenium import webdriver              # webdriver 모듈 불러오기\n",
    "URL = \"https://www.naver.com\"                # 네이버 URL을 URL 변수에 저장\n",
    "driver = webdriver.Chrome(\"chromedriver\")    # driver 객체 생성 \n",
    "driver.get(URL)                              # URL에 저장된 주소를 크롬에서 열기\n",
    "\n",
    "html = driver.page_source                    # HTML 소스 가져오기\n",
    "soup = BeautifulSoup(html, \"html.parser\")    # 데이터 변환(파싱)\n",
    "\n",
    "tag_list = soup.select(\"body p\")             # <body> 내 <p> 태그 추출\n",
    "for tag in tag_list:                        # tag_list 내 데이터를 하나씩 읽어오기\n",
    "    print(tag.text)                         # 텍스트만 출력\n",
    "driver.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 브라우저 제어하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium                               # selenium 모듈 불러오기\n",
    "from selenium import webdriver               # webdriver 모듈 불러오기\n",
    "from selenium.webdriver.common.by import By  # selenium의 by 클래스 불러오기\n",
    "URL = \"https://www.naver.com\"                 # 네이버 URL을 URL 변수에 저장\n",
    "driver = webdriver.Chrome(\"chromedriver\")     # driver 객체 생성 \n",
    "driver.get(URL)                               # URL에 저장된 주소를 크롬에서 열기\n",
    "btn = driver.find_element(By.XPATH, '//*[@id=\"account\"]/a')  # 네이버 로그인 버튼 선택\n",
    "btn.click()                                   # 로그인 버튼 클릭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium                              # selenium 모듈 불러오기\n",
    "from selenium import webdriver              # webdriver 모듈 불러오기\n",
    "from selenium.webdriver.common.by import By # selenium의 by 클래스 불러오기\n",
    "URL = \"https://www.naver.com\"                # 네이버 URL을 URL 변수에 저장\n",
    "driver = webdriver.Chrome(\"chromedriver\")    # driver 객체 생성 \n",
    "driver.get(URL)                              # URL에 저장된 주소를 크롬에서 열기\n",
    "input_tag = driver.find_element(By.CSS_SELECTOR, \"#query\")  # 검색창으로 이동\n",
    "input_tag.send_keys(\"박효신\")            # 검색창에 문자 입력\n",
    "input_tag.send_keys(\"\\n\")                    # Enter 키 실행\n",
    "driver.implicitly_wait(3)                    # 최대 3초간 대기(3초가 지나면 다음 코드 수행)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] 네이버 환율 정보 수집 후 CSV로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# 네이버 금융 웹 페이지로 이동하기\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")       # 크롬 드라이버 실행\n",
    "url = \"https://finance.naver.com/marketindex\"       # 사이트 URL\n",
    "driver.get(url)                                     # URL에 저장된 주소를 크롬에서 열기\n",
    "driver.switch_to.frame(\"frame_ex1\")                 # 환율 정보 프레임으로 전환\n",
    "\n",
    "# 파싱 및 태그 추출하기\n",
    "html = driver.page_source                           # 페이지 정보 가져오기\n",
    "soup = BeautifulSoup(html, \"html.parser\")           # HTML 파싱하기\n",
    "result = []                                         # 리스트 변수 생성\n",
    "currencys = soup.select(\"body > div > table > tbody > tr\")   # selector로 정보 가져오기\n",
    "\n",
    "for data in currencys :\n",
    "    country = data.select(\"td.tit > a\")[0].text.strip()      # 통화명 가져오기\n",
    "    exchange = data.select(\"td.sale\")[0].text                # 매매기준율 가져오기\n",
    "    result.append([country, exchange])                       # 리스트 변수에 저장\n",
    "\n",
    "# CSV 파일 만들기\n",
    "df = pd.DataFrame(result, columns = [\"통화명\", \"환율\"])                       # 표 컬럼 항목 추가\n",
    "df.to_csv(\"환율정보.csv\", encoding = \"cp949\", header = True, index = False)  # CSV 파일 생성\n",
    "driver.close()                                                                 # 웹 드라이버 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] 사업자등록번호 휴폐업 조회하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 불러오기\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 사업자등록번호 조회 웹 페이지 접속하기\n",
    "driver = webdriver.Chrome(\"chromedriver\")       # chromedriver.exe 파일 위치\n",
    "url =\"https://teht.hometax.go.kr/websquare/websquare.html?w2xPath=/ui/ab/a/a/UTEABAAA13.xml\"\n",
    "driver.get(url)                                 # 브라우저 열기\n",
    "time.sleep(3)                                   # 3초 대기\n",
    "\n",
    "# 사업자등록번호 엑셀 파일 읽어오기\n",
    "resultList = []                                 # 결과 저장 리스트 변수 생성\n",
    "df = pd.read_excel(\"사업자등록번호.xlsx\")       # 엑셀 파일 불러오기\n",
    "\n",
    "# 사업자등록번호를 입력해서 결과값 가져오기\n",
    "for regNo in df[\"Business_no\"] :                # for 문 생성\n",
    "    driver.find_element(By.CSS_SELECTOR, \"#bsno\").send_keys(regNo)  # 사업자등록번호 입력\n",
    "    driver.find_element(By.CSS_SELECTOR, \"#trigger5\").click()       # 조회하기 버튼 클릭\n",
    "    time.sleep(2)                               # 2초 대기시간 유지 \n",
    "    result = driver.find_element(By.CSS_SELECTOR, \"#grid2_cell_0_1\").text      # 결과값 가져오기\n",
    "    resultList.append(result)                   # 결과값을 reslutList에 저장하기\n",
    "\n",
    "# 조회 결과 저장하기\n",
    "df[\"result\"] = resultList                       # df에 열 추가\n",
    "df.to_excel(\"사업자등록번호 조회 결과.xlsx\", encoding=\"cp949\", index = False) # 엑셀로 저장\n",
    "driver.close()                                  # 드라이버 종료"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
